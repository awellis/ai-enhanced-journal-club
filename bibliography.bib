@online{cuiEffectsGenerativeAI2024a,
  type = {SSRN Scholarly Paper},
  title = {The {{Effects}} of {{Generative AI}} on {{High Skilled Work}}: {{Evidence}} from {{Three Field Experiments}} with {{Software Developers}}},
  shorttitle = {The {{Effects}} of {{Generative AI}} on {{High Skilled Work}}},
  author = {Cui, Zheyuan (Kevin) and Demirer, Mert and Jaffe, Sonia and Musolff, Leon and Peng, Sida and Salz, Tobias},
  date = {2024-09-03},
  number = {4945566},
  eprint = {4945566},
  eprinttype = {Social Science Research Network},
  location = {Rochester, NY},
  doi = {10.2139/ssrn.4945566},
  url = {https://papers.ssrn.com/abstract=4945566},
  urldate = {2024-11-09},
  abstract = {This study evaluates the impact of generative AI on software developer productivity by analyzing data from three randomized controlled trials conducted at Microsoft, Accenture, and an anonymous Fortune 100 electronics manufacturing company. These field experiments, which were run by the companies as part of their ordinary course of business, provided a randomly selected subset of developers with access to GitHub Copilot, an AI-based coding assistant that suggests intelligent code completions. Though each separate experiment is noisy, combined across all three experiments and 4,867 software developers, our analysis reveals a 26.08\% increase (SE: 10.3\%) in the number of completed tasks among developers using the AI tool. Notably, less experienced developers showed higher adoption rates and greater productivity gains.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Leon Musolff,Mert Demirer,Sida Peng,Sonia Jaffe,SSRN,The Effects of Generative AI on High Skilled Work: Evidence from Three Field Experiments with Software Developers,Tobias Salz,Zheyuan (Kevin) Cui},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-09T21:35:44.843Z},
  file = {/Users/andrew/Zotero/storage/HLE26E8E/Cui et al. - 2024 - The Effects of Generative AI on High Skilled Work Evidence from Three Field Experiments with Softwa.pdf}
}

@article{damischKeepYourFingers2010,
  title = {Keep {{Your Fingers Crossed}}!: {{How Superstition Improves Performance}}},
  shorttitle = {Keep {{Your Fingers Crossed}}!},
  author = {Damisch, Lysann and Stoberock, Barbara and Mussweiler, Thomas},
  date = {2010-07},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {21},
  number = {7},
  pages = {1014--1020},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797610372631},
  url = {https://journals.sagepub.com/doi/10.1177/0956797610372631},
  urldate = {2024-11-10},
  abstract = {Superstitions are typically seen as inconsequential creations of irrational minds. Nevertheless, many people rely on superstitious thoughts and practices in their daily routines in order to gain good luck. To date, little is known about the consequences and potential benefits of such superstitions. The present research closes this gap by demonstrating performance benefits of superstitions and identifying their underlying psychological mechanisms.Specifically,Experiments 1 through 4 show that activating good-luck-related superstitions via a common saying or action (e.g., “break a leg,” keeping one’s fingers crossed) or a lucky charm improves subsequent performance in golfing, motor dexterity, memory, and anagram games. Furthermore, Experiments 3 and 4 demonstrate that these performance benefits are produced by changes in perceived self-efficacy. Activating a superstition boosts participants’ confidence in mastering upcoming tasks, which in turn improves performance. Finally, Experiment 4 shows that increased task persistence constitutes one means by which self-efficacy, enhanced by superstition, improves performance.},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-10T13:18:44.255Z},
  file = {/Users/andrew/Zotero/storage/6J7976CL/Damisch et al. - 2010 - Keep Your Fingers Crossed! How Superstition Improves Performance.pdf}
}

@article{dellacquaNavigatingJaggedTechnological2023,
  title = {Navigating the {{Jagged Technological Frontier}}: {{Field Experimental Evidence}} of the {{Effects}} of {{AI}} on {{Knowledge Worker Productivity}} and {{Quality}}},
  shorttitle = {Navigating the {{Jagged Technological Frontier}}},
  author = {Dell'Acqua, Fabrizio and McFowland, Edward and Mollick, Ethan R. and Lifshitz-Assaf, Hila and Kellogg, Katherine and Rajendran, Saran and Krayer, Lisa and Candelon, François and Lakhani, Karim R.},
  date = {2023},
  journaltitle = {SSRN Electronic Journal},
  shortjournal = {SSRN Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4573321},
  url = {https://www.ssrn.com/abstract=4573321},
  urldate = {2024-11-09},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-09T22:57:23.795Z},
  file = {/Users/andrew/Zotero/storage/DIJN2GJ2/Dell'Acqua et al. - 2023 - Navigating the Jagged Technological Frontier Field Experimental Evidence of the Effects of AI on Kn.pdf}
}

@article{gelmanProblemsPValuesArea,
  title = {The {{Problems With P-Values}} Are Not {{Just With P-Values}}},
  author = {Gelman, Andrew},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-09T22:12:40.640Z},
  file = {/Users/andrew/Zotero/storage/92EKCRSP/Gelman - The Problems With P-Values are not Just With P-Values.pdf}
}

@article{ioannidisWhyMostPublished2005a,
  title = {Why {{Most Published Research Findings Are False}}},
  author = {Ioannidis, John P. A.},
  date = {2005-08-30},
  journaltitle = {PLoS Medicine},
  shortjournal = {PLoS Med},
  volume = {2},
  number = {8},
  pages = {e124},
  issn = {1549-1676},
  doi = {10.1371/journal.pmed.0020124},
  url = {https://dx.plos.org/10.1371/journal.pmed.0020124},
  urldate = {2024-11-10},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-10T13:53:47.649Z},
  file = {/Users/andrew/Zotero/storage/ADXR7YYP/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf}
}

@online{liangCanLargeLanguage2023,
  title = {Can Large Language Models Provide Useful Feedback on Research Papers? {{A}} Large-Scale Empirical Analysis},
  shorttitle = {Can Large Language Models Provide Useful Feedback on Research Papers?},
  author = {Liang, Weixin and Zhang, Yuhui and Cao, Hancheng and Wang, Binglu and Ding, Daisy and Yang, Xinyu and Vodrahalli, Kailas and He, Siyu and Smith, Daniel and Yin, Yian and McFarland, Daniel and Zou, James},
  date = {2023-10-03},
  eprint = {2310.01783},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.01783},
  urldate = {2024-11-11},
  abstract = {Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4’s feedback through two large-scale studies. We first quantitatively compared GPT-4’s generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85\% for Nature journals, 39.23\% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58\% for Nature journals, 35.25\% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers (i.e., rejected ICLR papers; average overlap 43.80\%). We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4\%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4\% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations. For example, GPT-4 tends to focus on certain aspects of scientific feedback (e.g., ‘add experiments on more datasets’), and often struggles to provide in-depth critique of method design. Together our results suggest that LLM and human feedback can complement each other. While human expert review is and should continue to be the foundation of rigorous scientific process, LLM feedback could benefit researchers, especially when timely expert feedback is not available and in earlier stages of manuscript preparation before peer-review.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-11T00:25:21.447Z},
  file = {/Users/andrew/Zotero/storage/MJZD7ADE/Liang et al. - 2023 - Can large language models provide useful feedback on research papers A large-scale empirical analys.pdf}
}

@article{munafoManifestoReproducibleScience2017,
  title = {A Manifesto for Reproducible Science},
  author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Percie du Sert, Nathalie and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
  date = {2017-01-10},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {1},
  number = {1},
  pages = {1--9},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-016-0021},
  url = {https://www.nature.com/articles/s41562-016-0021},
  urldate = {2024-11-10},
  abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
  langid = {english},
  keywords = {Social sciences},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-10T14:59:06.865Z},
  file = {/Users/andrew/Zotero/storage/77Q5JRKX/Munafò et al. - 2017 - A manifesto for reproducible science.pdf}
}

@online{sharmaUnderstandingSycophancyLanguage2023,
  title = {Towards {{Understanding Sycophancy}} in {{Language Models}}},
  author = {Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R. and Cheng, Newton and Durmus, Esin and Hatfield-Dodds, Zac and Johnston, Scott R. and Kravec, Shauna and Maxwell, Timothy and McCandlish, Sam and Ndousse, Kamal and Rausch, Oliver and Schiefer, Nicholas and Yan, Da and Zhang, Miranda and Perez, Ethan},
  date = {2023-10-27},
  eprint = {2310.13548},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2310.13548},
  url = {http://arxiv.org/abs/2310.13548},
  urldate = {2024-09-08},
  abstract = {Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.6,Statistics - Machine Learning},
  file = {/Users/andrew/Zotero/storage/W3PV4TMS/Sharma et al. - 2023 - Towards Understanding Sycophancy in Language Models.pdf;/Users/andrew/Zotero/storage/DIYLTKRG/2310.html}
}

@online{spilliasEvaluatingGenerativeAI2024,
  title = {Evaluating {{Generative AI}} to {{Extract Qualitative Data}} from {{Peer-Reviewed Documents}}},
  author = {Spillias, Scott and Ollerhead, Katherine and Andreotta, Matthew and Annand-Jones, Ruby and Boschetti, Fabio and Duggan, Joseph and Karcher, Denis and Paris, Cecile and Shellock, Rebecca and Trebilco, Rowan},
  date = {2024-08-26},
  eprinttype = {Research Square},
  issn = {2693-5015},
  doi = {10.21203/rs.3.rs-4922498/v1},
  url = {https://www.researchsquare.com/article/rs-4922498/v1},
  urldate = {2024-11-08},
  abstract = {Uptake of AI tools in knowledge production processes is rapidly growing. Here, we explore the ability of generative AI tools to reliably extract qualitative data from peer-reviewed documents. Specifically, we evaluate the capacity of multiple AI tools to analyse literature and extract relevant information for a systematic literature review, comparing the results to those of human reviewers. We address how well AI tools can discern the presence of relevant contextual data, whether the outputs of AI tools are comparable to human extractions, and whether the difficulty of question influences the performance of the extraction. While the AI tools we tested (GPT4-Turbo and Elicit) were not reliable in discerning the presence or absence of contextual data, at least one of the AI tools consistently returned responses that were on par with human reviewers. These results highlight the utility of AI tools in the extraction phase of evidence synthesis for supporting human-led reviews and underscore the ongoing need for human oversight.},
  pubstate = {prepublished},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-08T19:49:24.448Z},
  file = {/Users/andrew/Zotero/storage/2UJPEMEC/Spillias et al. - 2024 - Evaluating Generative AI to Extract Qualitative Data from Peer-Reviewed Documents.pdf}
}

@article{toner-rodgersArtificialIntelligenceScientific,
  title = {Artificial {{Intelligence}}, {{Scientific Discovery}}, and {{Product Innovation}}},
  author = {Toner-Rodgers, Aidan},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-11-09T21:09:14.350Z},
  file = {/Users/andrew/Zotero/storage/SZ2BM2CS/Toner-Rodgers - Artificial Intelligence, Scientific Discovery, and Product Innovation.pdf}
}

@article{yanaiItTakesTwo2024,
  title = {It Takes Two to Think},
  author = {Yanai, Itai and Lercher, Martin J.},
  date = {2024-01},
  journaltitle = {Nature Biotechnology},
  shortjournal = {Nat Biotechnol},
  volume = {42},
  number = {1},
  pages = {18--19},
  publisher = {Nature Publishing Group},
  issn = {1546-1696},
  doi = {10.1038/s41587-023-02074-2},
  url = {https://www.nature.com/articles/s41587-023-02074-2},
  urldate = {2024-10-16},
  langid = {english},
  keywords = {Education,Lab life},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2024-10-16T20:30:14.063Z},
  file = {/Users/andrew/Zotero/storage/7M7Y5L8T/Screenshot 2024-10-16 at 22.30.57.png}
}
